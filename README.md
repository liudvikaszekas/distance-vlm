# DistanceVLM

Vision-Language Models (VLMs) have achieved impressive results across multimodal tasks, but they continue to face challenges in accurately estimating spatial distances within real-world images. In this report, we introduce DistanceVLM, an approach to enhance the spatial reasoning capabilities of existing VLMs by fine-tuning them on a new, large-scale dataset of street-view images with precisely annotated distance information. Our dataset is automatically generated by integrating street-view imagery with geospatial data from OpenStreetMap, resulting in precise ground-truth distance labels for a variety of urban objects. Through this approach, we generated a high-quality, diverse dataset consisting of approximately 2,000 images, each annotated with accurate spatial measurements. By fine-tuning several popular VLMs using this dataset, we significantly improved their capability to estimate real-world distances. Experimental results demonstrate substantial gains in spatial reasoning, with notable improvements in accuracy, correlation, and reduction of estimation outliers. This approach also enhanced their reliability in consistently providing accurate predictions, with most VLMs giving an answer all the time. Our findings suggest that enriching VLM training data with realistic spatial context effectively enhances spatial reasoning abilities without requiring extensive architectural modifications or complex reasoning methods.

## Info

The most important scripts are inside scripts/final_scripts folder. The finetuning of the VLMs is based on the lmms-finetune repository: https://github.com/zjysteven/lmms-finetune.