{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff0536c-8f1d-4a6f-90ff-f28e975aa778",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (1991594926.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    DISTRIBUTED_ARGS=\"\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "export HF_HOME=/hpi/fs00/scratch/liudvikas.zekas/.cache\n",
    "NUM_GPUS=2\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nnodes=1 \\\n",
    "    --nproc_per_node ${NUM_GPUS} \\\n",
    "    --rdzv_backend c10d \\\n",
    "    --rdzv_endpoint localhost:0\n",
    "\"\n",
    "\n",
    "# arguments that are very likely to be changed\n",
    "# according to your own case\n",
    "MODEL_ID=llava-1.5-7b                                   # model id; pick on by running `python supported_models.py`\n",
    "TRAIN_DATA_PATH=./dataset_new/train.json  # path to the training data json file\n",
    "EVAL_DATA_PATH=./dataset_new/val.json    # path to the evaluation data json file (optional)\n",
    "IMAGE_FOLDER=/                      # path to the image root folder; if provided, the image paths in the json should be relative\n",
    "VIDEO_FOLDER=/                  # path to the video root folder; if provided, the video paths in the json should be relative\n",
    "NUM_FRAMES=8                                            # how many frames are sampled from each video\n",
    "\n",
    "TRAIN_VISION_ENCODER=False                              # whether train the vision encoder\n",
    "USE_VISION_LORA=False                                   # whether use lora for vision encoder (only effective when `TRAIN_VISION_ENCODER` is True)\n",
    "TRAIN_VISION_PROJECTOR=False                            # whether train the vision projector (only full finetuning is supported)\n",
    "\n",
    "USE_LORA=True                                           # whether use lora for llm\n",
    "Q_LORA=False                                            # whether use q-lora for llm; only effective when `USE_LORA` is True\n",
    "LORA_R=8                                                # the lora rank (both llm and vision encoder)\n",
    "LORA_ALPHA=8                                            # the lora alpha (both llm and vision encoder)\n",
    "\n",
    "RUN_ID=${MODEL_ID}_lora-${USE_LORA}_qlora-${Q_LORA}     # a custom run id that determines the checkpoint folder and wandb run name\n",
    "\n",
    "DS_STAGE=zero3                                          # deepspeed stage; < zero2 | zero3 >\n",
    "PER_DEVICE_BATCH_SIZE=2                                 # batch size per GPU\n",
    "GRAD_ACCUM=1                                            # gradient accumulation steps\n",
    "NUM_EPOCHS=5                                            # number of training epochs\n",
    "\n",
    "LR=2e-5                                                 # learning rate\n",
    "MODEL_MAX_LEN=1024                                       # maximum input length of the model\n",
    "\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS lmms-finetune/train.py \\\n",
    "    --model_id $MODEL_ID \\\n",
    "    --data_path $TRAIN_DATA_PATH \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --image_folder $IMAGE_FOLDER \\\n",
    "    --video_folder $VIDEO_FOLDER \\\n",
    "    --num_frames $NUM_FRAMES \\\n",
    "    --output_dir /hpi/fs00/scratch/liudvikas.zekas/checkpoints/$RUN_ID \\\n",
    "    --report_to wandb \\\n",
    "    --run_name $RUN_ID \\\n",
    "    --deepspeed ./lmms-finetune/ds_configs/${DS_STAGE}.json \\\n",
    "    --bf16 True \\\n",
    "    --num_train_epochs $NUM_EPOCHS \\\n",
    "    --per_device_train_batch_size $PER_DEVICE_BATCH_SIZE \\\n",
    "    --per_device_eval_batch_size $PER_DEVICE_BATCH_SIZE \\\n",
    "    --gradient_accumulation_steps $GRAD_ACCUM \\\n",
    "    --eval_strategy \"epoch\" \\\n",
    "    --save_strategy \"epoch\" \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate ${LR} \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 False \\\n",
    "    --model_max_length $MODEL_MAX_LEN \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --train_vision_encoder $TRAIN_VISION_ENCODER \\\n",
    "    --use_vision_lora $USE_VISION_LORA \\\n",
    "    --train_vision_projector $TRAIN_VISION_PROJECTOR \\\n",
    "    --use_lora $USE_LORA \\\n",
    "    --q_lora $Q_LORA \\\n",
    "    --lora_r $LORA_R \\\n",
    "    --lora_alpha $LORA_ALPHA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba38980-d79c-4631-8330-3275f2f9f415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train.json: total items = 1366, new file items = 14\n",
      "Processed val.json: total items = 293, new file items = 3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder = \"dataset_new\"\n",
    "\n",
    "# List of file names to process\n",
    "files = [\"train.json\", \"val.json\"]\n",
    "\n",
    "for filename in files:\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Calculate the number of items (at least one)\n",
    "    n_items = max(1, math.ceil(len(data) * 0.01))\n",
    "    \n",
    "    # Take only the first 1% of the items\n",
    "    new_data = data[:n_items]\n",
    "    \n",
    "    # Define new filename: e.g., train_new.json\n",
    "    new_filename = filename.replace(\".json\", \"_new.json\")\n",
    "    new_file_path = os.path.join(folder, new_filename)\n",
    "    \n",
    "    with open(new_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Processed {filename}: total items = {len(data)}, new file items = {n_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a5956-69df-474e-92eb-a3e7266f6756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
